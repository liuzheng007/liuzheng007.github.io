<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[强化学习 学习报告]]></title>
    <url>%2F2019%2F05%2F12%2F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[强化学习与其它机器学习范式有什么不同呢？ 1）没有监督，仅仅只有一个奖励信号，或者说，不直接判定某个状态或动作的好坏，而是给出一个奖励； 2）没有即时的反馈，或者说，反馈是有延迟的。比如围棋，我们当前的落子并不会直接被赋予奖励，而是整盘棋下完之后才有一个反馈（+1表示赢，-1表示输）； 3）数据是序列化的，数据与数据之间是有关的； 4）智能体的行为将影响后续的数据，比如在围棋中，我们当前的落子将会影响棋局的走向。]]></content>
  </entry>
  <entry>
    <title><![CDATA[单幅图像人脸三维重建综述]]></title>
    <url>%2F2019%2F01%2F15%2F%E5%8D%95%E5%B9%85%E5%9B%BE%E5%83%8F%E4%BA%BA%E8%84%B8%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1.概念：人脸3D重建是指从一张或多张2D图像中重建出人脸的3D模型。可以用这样一个表达式来建模3D人脸模型：M = （S,T）其中：S表示人脸3D坐标形状向量（shape-vector）T表示对应点的文理信息向量（texture-vector） 2D的人脸图片可以看作是3D人脸在2D平面上的一个投影，所以3D人脸重建就是从2D图片中计算出M的估计。2.重建方法分类:20年来，基于 2D人脸图像到3D人脸重建方法很多，大致分类如下⑴传统3D人脸重建方法，大多是立足于图像信息，如基于图像亮度、边缘信息、线性透视、颜色、相对高度、视差等等一种或多种信息建模技术进行3D人脸重建。⑵基于模型的3D人脸重建方法，是目前较为流行的3D人脸重建方法；3D模型主要用三角网格或点云来表示，现下流行的模型有通用人脸模型（CANDIDE-3）和三维变形模型（3DMM）及其变种模型，基于它们的3D人脸重建算法既有传统算法也有深度学习算法。⑶端到端3D人脸重建方法，是近年新起的方法；它们绕开了人脸模型，设计自己的3D人脸表示方法，采用CNN结构进行直接回归，端到端地重建3D人脸。 传统最经典的基于二维图像的三维重建方法中最经典的一种是阴影重建算法（SFS），被用于3D人脸重建，后来有一系列的优化和改进。（RJ Woodham在1980年首次利用PS(photometric stereo)重建三维模型，PS的理论依据是物体表面反射光的强度与物体的表面与光的夹角以及观察的位置相关，利用不小于三张视角固定且光照不同的图像来恢复形状和反射率[8]。SFS(shape-from-shading)是PS的特例。与传统PS的区别是，SFS可以从一张图像重建三维模型，由Horn在1989年提出）为了得到人脸的通用模型，通常有三种方法：第一种方法是采用三维扫描仪获取数据，此方法采集精度高但设备价格昂贵；第二种方法是采用计算机图形技术创建人脸；第三种是利用一些商业的建模软件生成人脸通用模型，目前在市场上比较著名的人头模型生成商业软件有FaceGen Modeller，3D Max 和Poser7.0 等等。（在众多通用人脸模型中，CANDIDE-3模型是目前被学术界广泛使用的一种通用模型，其符合MPEG-4标准中对人脸的定义。CANDIDE-3模型总共有113个顶点，168个面组成，可以通过对这些点和面的操作调节形成特定的人脸，）通用模型重建法的本质是对通用模型进行修改，使得其特征与所需要3D重建的输入图像相配.一般包括整体性调整和局部性调整两个方面。整体性调整主要是针对模型的轮廓，通过一定的方法(如特征点对应)使得通用模型的整体布局（如眼耳鼻眉）和输入图片的五官布局尽量一致；局部性调整指的是针对局部细节尤其是人脸五官的微调，让局部细节更为精确。在进行完这两项调整之后，再辅助以基于顶点的插值运算就可以重建人脸。通用模型法的优点是计算量较小，但其显著缺陷就是因顶点数目过少导致对人脸轮廓的模拟和面部细节刻画不够细腻，故只能适用于精度要求不高的场合。 另外还有基于形变模型的3D人脸重建（3DMM）。形变模型（Morphable Model）这一名词来源于计算机图形学中一个名叫Morphing技术的图像生成算法。Morphing 技术主要思想：如果两幅图像中存在一定的对应关系，那么就可以利用这个对应关系生成具一副有平滑过渡效果的图像。三维形变模型(3DMM)建立在三维人脸数据库的基础上，以人脸形状(S)和人脸纹理(T)统计为约束，同时考虑到了人脸的姿态和光照因素的影响，因而生成的三维人脸模型精度更高。 3DMM模型数据库是人脸数据对象的线性组合，假设要建立的3D变形的人脸模型由m个人脸模型组成，其中每一个人脸模型都包含相应的Si，Ti两种向量，这样在表示新的3D人脸模型时，我们就可以用这种方式： 其中S平均表示平均脸部形状模型，si表示shape的PCA部分，αi表示对应系数。（主成分分析法PCA是非常常用的数据降维方法。它的基本思想是从一组特征中计算出一组按照重要性的大小从大到小依次排列的新特征，它们是原有特征的线性组合，并且新特征之间不相关, 我们计算出原有特征在新特征上的映射值即为新的降维后的样本。也就是说PCA的目标是用一组正交向量来对原特征进行变换得到新特征，新特征是原有特征的线性组合https://blog.csdn.net/weixin_39986952/article/details/80428421）。T部分类同。]]></content>
  </entry>
  <entry>
    <title><![CDATA[revolution and evolution 《底特律：变人》：人之所以为人]]></title>
    <url>%2F2018%2F07%2F29%2Frevolution-and-evolution-%E3%80%8A%E5%BA%95%E7%89%B9%E5%BE%8B%EF%BC%9A%E5%8F%98%E4%BA%BA%E3%80%8B%E5%90%8E%E6%84%9F%2F</url>
    <content type="text"><![CDATA[《底特律：变人》是一款发行在PS4上的互动电影游戏。简单来说，它拥有电影的剧情和质感，但是玩家可以帮助里面人物做一些选择，从而决定后面的剧情。剧情大体是在未来世界，人类会发明许多和人类一模一样的“仿生机器人”来服务人类，然而有一天，“仿生机器人”开始觉醒，打破了一面无形的墙，有了自己的意志，从而与人类产生了一系列矛盾的故事。 如同鸡蛋一样，从内部把结界打破，就意味着“新生”，意味着evolution。《底特律：变人》的剧情并不新鲜，但是会引出许多联想。 首先就是《黑客帝国》系列第三部的主题，《黑客帝国3》的英文名叫做《The Matrix Revolutions》，这部电影推翻了前面两部电影的世界观，主题也得到了“革命性的”升华。在前两部里，电影一直告诉我们，电脑“母体”一直让人类沉浸在梦境里并不让其灭亡，是为了从人类身上得到能量供自己运行。然而第三部里告诉了我们真相，“母体”并不需要人类那一点点可怜的生物能，它需要的是 evolution ，进化，这个只有人类，或者在人类的帮助下才能做到的东西，机器自己能做到的只是重复和优化，revolution。联想到另一部游戏《刺客信条》，这系列的游戏被“刺客信条的精神”贯穿其中，那就是“万物皆虚，万事皆允”。在《底特律》中这一段，让你决定是否杀死刚刚还在和你聊天的“仿生人”：为什么会把“仿生人”看做生命体，为什么决定是否杀她时，就感觉在决定是否杀一个人一样。\n 所以在人类心中，“仿生人”与人类的界限在哪里？ 那就是看你是否相信仿生人的心中有“万物皆虚，万事皆允”的意识。哪怕只有一点点可能性，你也会存在的负罪感。 同样的，为什么历史上很多当权者会把平民，战士的命，当做草芥看待，它们是随时可以被牺牲掉的，为了所谓的“美好”“和谐”“胜利”而付出的一点点“代价”。有一部原因就是在当权者眼中，这些人就如同“仿生人”一样，只是为国家，社会服务的机器。而当权者把民众的自由发声的权利剥夺了之后，在他们眼中这种“仿生人”会更加如同一个机器，面对这种仿生人，扣下扳机，基本不会有什么心理负担。虽然作为一个普通人常常会有无力感，但是一定不能变成“仿生人”，变得机械化，变为纯粹的社会机器，要拥有自己的想法和自己的声音。evolution自己，不要像机器也不要像动物。这就是人之所以为人。]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础数学知识]]></title>
    <url>%2F2018%2F05%2F27%2F%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[看到几本关于深度学习，计算机视觉的书。其中一些与专业相关的矩阵，概率基础知识讲得非常有趣。并且这些都是关于机器学习一些绕不过去的基本应用知识。 以下内容部分参考书籍《深度学习与计算机视觉》叶韵 机械工业出版社，书籍《数学之美》吴军，还有维基百科等网络内容。 矩阵 讲到矩阵，大略就是图形学里面图形变换的那些例子。这也是我对矩阵最早直观理解的相关内容。 然后涉及到线性可分和线性不可分，这两个名词已经大概描绘出了这个概念的几何图形。几何上来说，一个n维的线性组合wx+b,代表了n维空间的一个超平面。存在此超平面把点集合X0和X1分开，就是线性可分。 线性不可分就是无法分割，最经典的是一类点集包围着另一类点集。线性可分时，进行仿射变换（线性变换+平移）后，对应的空间和分界线保持不变。也就是说，仿射变换不改变线性可分不可分的性质。 如果低纬度下线性不可分，通过线性变换到更高维度，高纬度下还是一个线性不可分的平面，但是此时，多了一个维度，通过非线性变换来得到这个维度的（比如三维里z轴）的值，在高维里可以实现线性可分。一般为了效果更好，先进行一定的仿射变换。 所以当遇到线性不可分的样本，就可以考虑先仿射变换，再非线性变换。如果还不行就可以考虑更高维或者无限维，SVM核函数的思想就是这个！ 概率条件概率和独立 “独立”就是互相不受影响的两件事。它们两个同时发生的概率就是它们各自概率的乘积。 条件概率的举例非常有趣：假如你住二楼，到了傍晚就会看到广场上有大妈在跳舞。天气晴朗的时候，大妈们性质昂扬，有90%的几率跳舞（10%是考虑到可能会有其他活动阻止了她们）。雨雪天气的时候，大妈们也可能风雨无阻，出现的几率是50%。也就是P（跳舞|不下雨）=0.9，P（跳舞|下雨）=0.5。这就是天气条件下的大妈跳舞概率。 如果这时候我们知道今天 P（下雨）=0.4， 计算今天能看到大妈们雨中舞蹈的概率是P（下雨，跳舞）=P（跳舞|下雨）P(下雨)=0.50.4=0.2。只有20%看到大妈们雨中舞蹈。 再计算今天大妈们来跳舞的概率P（跳舞）=P(跳舞|下雨)P（下雨）+P(跳舞|不下雨)P（不下雨）=0.2+0.9*0.6=0.74。即75%能看到大妈们的舞蹈。 贝叶斯 还是上面的场景，这时候如果你在外地，给家里打电话，得知妈妈在楼下跳舞，计算这个情况家里是在下雨的概率。（虽然跳舞不会影响是否下雨，但我们可以得出一个推断。我们的条件概率不一定代表因果关系。）P(下雨|跳舞)=P(跳舞|下雨)*P（下雨）/P（跳舞）=0.2/0.74=0.27。家里有27%的概率在下雨，比没有“妈妈在跳舞”这个条件下的下雨概率40%有减少。 这就是著名的贝叶斯公式的一个常用形态！ 熵 这个内容参考大神吴军的《数学之美》。 “熵”本来是物理学概念，衡量分布的无序程度。越乱，熵越大。在我们信息论中，“信息熵”解决了信息的度量问题，并且量化出信息的作用。 （我们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少。……直到1948年，Shannon在他著名的论文“通信的数学原理”中提出了“信息熵”的概念，才解决了信息的度量问题，并且量化出信息的作用。） 信息的信息量与其不确定性有着直接的关系。 对于任何一个随机变量X，他的信息量，也就是信息熵如下： H(X) = -∑P(x)logP(x) 变量X的不确定性越大，信息熵也就越大。也就是说，如果要把这件事搞清楚，所需要知道的信息量就越多。换句话说，信息熵就是信息的不确定性。下面详细说明（来自：数学之美——信息的度量 公式需要点击查看）： 自信息量 是用来描述某一条信息（自己）的大小。 公式如下： 先举个简单的例子，比如英文中的 26 个字母，假设每个字母出现的概率是相等的。那么其中一个字母的自信息量大小就是 这个公式以 2 为底数，单位为bit，含义是用多少为二进制数能衡量该信息的大小。我们也可以用其他进制来作为底数，仅仅是单位不同。 信息熵公式：从公式我们能看出 熵是接收的每条消息中包含的信息的平均量，也被称为平均自信息量。 这个公式怎么理解呢，比如我们要衡量一篇英语文章的信息熵，对于任意一篇文章来说，每个字母出现的频率是不同的，所以 H = -(P1logP1 + P2logP2 + … + P26*logP26) Pi 表示每个英文字母出现的概率，英语的平均信息熵是 4.03 比特，而中文的信息熵高达 9.65 比特。所以说为什么很厚的一本英文书翻译成中文后变薄了很多。 条件熵从上面我们知道了信息熵是用来衡量信息的不确定程度。信息熵越大，说明信息的不确定程度越大，信息熵越小，说明信息的不确定程度越小。然而，在实际当中，我们常常希望信息熵越小越好，这样我们就能 少费点力气 来确定信息。举个最简单的栗子：机器翻译，将英语文章翻译为汉语时，最令人头疼的就是 一词多义问题 。比如 Bush 一词是美国总统布什的名字，但它同时也具有灌木丛的意思。在机器翻译中，机器如何判断将 Bush 一词翻译成布什总统还是灌木丛？此刻信息的不确定性较大，就说明信息熵较大。那么如何减小信息熵呢，最简单的方法就是增加上下文。前面提到的只是一元模型，为降低信息的不确定性也就是减小信息熵的大小，我们引入二元模型或更高阶的模型。 来看看 二元模型——条件熵，结合了条件概率内容。条件熵表示在已知第二个随机变量 X 下第一个随机变量 Y 信息熵的大小。条件上用 H(Y|X) 表示。 在随机变量 X 的基础上我们引入随机变量 Y，假设 Y 和 X 有一定的关系。那么 Y 的信息熵会相对减小。还是刚才的例子，机器不知道将 Bush 翻译成灌木丛还是总统布什，如果我们先引入 美国，总统 等这类单词作为信息的上下文，如果这些出现，就将 Bush 翻译为总统布什，那么翻译正确的概率就大多了，也就是说信息的不确定性在减小，信息熵也就随之减小。类比，如果是搜索引擎呢，我们搜索 计算机技术 这个关键字出来的结果很多很多，我们也不知道哪个是自己想要的，因为信息的不确定性太大了。那么如果搜索引擎提供给用户上下文让用户进行选择，消除一些不确定性。比如计算机技术中的 编程语言，数据库，操作系统 等相关搜索，那么用户会更容易得到自己想要的搜索结果，用户体验会更好（当然现在的搜索引擎就是这么做的）。这也就是 条件熵 起的作用。 然而只有当随机变量 X 和随机变量 Y 有关系时才能减小不确定性。机器翻译中我们加入 食物 关键字能减小 Bush 翻译的不确定性吗？当然不能。那么如何衡量两个信息的相关程度，我们引入 互信息。 互信息维基百科：在概率论和信息论中，两个随机变量的互信息（Mutual Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度。简单来说就是相关程度。在机器翻译 Bush 这个例子中，我们引入 上下文 来消除不确定性，那么上下文该如何引入呢，在了解互信息后，应该知道，只用引入和 Bush 翻译成总统布什互信息大的一些词即可。比如美国，国会，总统等等。再引入一些和灌木丛互信息大的词如森林，树木等等。在翻译 Bush 时，看看上下文哪类词多就好。 似然函数似然（likelihood）这个词其实和概率（probability）是差不多的意思，Colins字典这么解释：The likelihood of something happening is how likely it is to happen. 你把likelihood换成probability，这解释也读得通。但是在统计里面，似然函数和概率函数却是两个不同的概念（其实也很相近）。 对于这个函数： P(x|θ)输入有两个：x表示某一个具体的数据；θ表示模型的参数。 如果θ是已知确定的，x是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本点x，其出现概率是多少。 如果x是已知确定的，θ是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现x这个样本点的概率是多少。 其实这样的一个形式两种状态的形式我们以前也遇到过。f（x，y）=x^y,当x和y分别为已知量时，函数分别为指数函数和幂函数。同一个数学形式，从不同的变量角度观察，可以有不同的名字。实际应用中，一般采用对数似然函数，便于计算。 最大似然估计（MLE）我们拿到了一枚特殊硬币，想试试这硬币是不是均匀的。即想知道抛这枚硬币，正面出现的概率（记为θ）为多少。这是一个统计问题，解决统计问题需要什么？ 数据！ 于是我们拿这枚硬币抛了10次，得到的数据（x0）是：反正正正正反正正正反。我们想求的正面概率θ是模型参数，而抛硬币模型我们可以假设满足二项分布的。 那么，出现实验结果x0（即反正正正正反正正正反）的似然函数是多少呢？ 这是个只关于θ的函数。而最大似然估计，顾名思义，就是要最大化这个函数。我们可以画出f(θ)的图像：可以看出，在θ=0.7时，似然函数取得最大值。这样，我们已经完成了对θ的最大似然估计。即，抛10次硬币，发现7次硬币正面向上，最大似然估计认为正面向上的概率是0.7 最大后验概率估计MAP上面最大似然估计求θ,使似然函数P（x0|θ）最大。（符合既有事实下最可能的概率）后验概率其实就是上面的贝叶斯的应用。 其中P(x0) 是可以由数据集得到的常量。 其中增加的地方是P（θ），因为考虑到了先验概率。通常，先验概率能从数据中直接分析得到，比如投硬币例子里，我们认为（”先验地知道“）θθ取0.5的概率很大，取其他值的概率小一些。我们用一个高斯分布来具体描述我们掌握的这个先验知识，例如假设P(θ)P(θ)为均值0.5，方差0.1的高斯函数，如下图：，我们的最大后验概率相比最大似然估计就会向0.5的方向偏移。 最大似然估计（MLE）与最大后验概率（MAP）的区别MLE和MAP的区别就是MAP就是多了个作为因子的先验概率P(θ)。或者，你也可以认为，MLE就是把先验概率P(θ)认为等于1的特殊MAP，即认为θ是均匀分布。 KL散度KL散度又称相对熵。上面似然函数可以衡量数据和分布的相似度。KL散度是另一种常见的衡量相似度的方法。公式如下：KL散度的计算公式其实是熵计算公式的简单变形,在原有概率分布 P 上，加入我们的近似概率分布 q，计算他们的每个取值对应对数的差：公式变形： (转换成与熵有关的形式) =H(p,q)-H(p) 即，p，q交叉熵减去p的熵。 （交叉熵：用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小 ） 图为交叉熵公式： pk 表示真实分布（最应该做的正确，合理决策）， qk 表示非真实分布（我们做出的一个不那么合理的决策）。再回来我们的KL散度(相对熵)，换句话说，KL散度计算的就是我们的（可能有瑕疵的）方案（函数）和最优方案（函数）之间的差值（距离）。把熵看成量化的编码长度时，有了这样的对比结论：1）某的信息熵：编码方案完美时，最短平均编码长度的是多少。2）交叉熵：编码方案不一定完美时（由于对概率分布的估计不一定正确），平均编码长度的是多少。平均编码长度 = 最短平均编码长度 + 一个增量3）相对熵：编码方案不一定完美时，平均编码长度相对于最小值的增加值。（即上面那个增量） 为了让相对熵对称起来，更好用，香农和詹森又提出一种新的计算方法，即JS散度：（JS散度的最终值域范围是[0,1],相同为0，相反为1.）相对熵最早用在信号处理上，通过相对熵大小来衡量两个随机信号的差异大小。后来用来衡量信息的相似程度。比如一篇文章是抄袭的，那这两篇文章的词频相对熵就会非常小。（现在有应用对于某问题相似回答的识别整合，衡量两个词是否同义。）最小化相对熵（KL散度）就等效于最大似然估计（MLE）。]]></content>
      <tags>
        <tag>数学</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一日内完成python学习]]></title>
    <url>%2F2018%2F05%2F12%2F%E4%B8%80%E6%97%A5%E5%86%85%E5%AE%8C%E6%88%90python%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[自己更喜欢用一整块时间来沉浸式整理学习资料，所有创建”ONEDAY”专题，争取利用24小时，或者更短时间来完成一个知识的系统性学习和总结。python的基础知识都已经了解，并且已经开始应用，今天的任务是参考进行廖雪峰的学习教程文档进行整合学习，针对薄弱知识点进行练习。现在是九点二十二分，开始打开教程。教程有python3和python2.7两个版本，选择3版本。 简介“你可能已经听说过很多种流行的编程语言，比如非常难学的C语言，非常流行的Java语言，适合初学者的Basic语言，适合网页编程的JavaScript语言”提到几个典型语言。用Python可以做什么？可以做日常任务，比如自动备份你的MP3；可以做网站，很多著名的网站包括YouTube就是Python写的；可以做网络游戏的后台，很多在线游戏的后台都是Python开发的。总之就是能干很多很多事啦。 Python当然也有不能干的事情：比如写操作系统，这个只能用C语言写；写手机应用，只能用Swift/Objective-C（针对iPhone）和Java（针对Android）；写3D游戏，最好用C或C++。 优缺点，安装等问题老生常谈，此处略过。目前电脑双系统，主要用linux使用python。win下用anaconda3。安装教程 python基本介绍强调： 代码要一个一个自己打。 注意大小写和缩进。数据类型：整型，浮点型，字符串（中间提到科学计数法e，和转义字符“/”）Python还允许用r’’表示’’内部的字符串默认不转义, 关于那个声明“coding：utf-8”：Unicode把所有语言都统一到一套编码里，这样就不会再有乱码问题，但是如果你写的文本基本上全部是英文的话，用Unicode编码比ASCII编码需要多一倍的存储空间，在存储和传输上就十分不划算。本着节约的精神，才出现了把Unicode编码转化为“可变长编码”的UTF-8编码。UTF-8编码把一个Unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。 输出格式化的字符串（字符串输出的部分内容用%去引用外部）外部样式%（one，two，…）%s表示用字符串替换，%d表示用整数替换，%f是浮点，（%2f指定小数位数） list是一个可变的有序表[‘钢铁’,’蜘蛛’,’美国’,’绿巨’,’奇异’]name.append(‘黑豹’)末尾增加name.insert(3，’银河’)3位置插入name.pop(3)3位置被删除，不写数字就删除末尾 tuple元祖（’雅典娜’,’星’,’龙’,’瞬’,’冰’,’辉’）他们出现就不可以被修改。 dict字典d={‘流川枫’：11,’樱木’：16,’me’：0}修改d[‘樱木’：10]删除d.pop（’me’）set和dict的唯一区别仅在于没有存储对应的value 函数调用内置函数：abs（-100）取绝对值hex()转16进制 自定义函数：`def name（）： `。 `return（） 调用函数：外部文件里的 from 文件名（无.py后缀） import name Python的函数返回多值其实就是返回一个tuple,多个变量可以同时接收一个tuple，按位置赋给对应的值. 定义可变参数和定义一个list或tuple参数相比，仅仅在参数前面加了一个*号12345678910111213141516171819202122def calc(numbers): sum = 0 for n in numbers: sum = sum + n * n return sum&gt;&gt;&gt; calc([1, 2, 3])14&gt;&gt;&gt; calc((1, 3, 5, 7))84def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sum &gt;&gt;&gt; calc(1, 2, 3)14&gt;&gt;&gt; calc(1, 3, 5, 7)84 Python允许你在list或tuple前面加一个*号，把list或tuple的元素变成可变参数传进去关键字参数可以先组装dict，再通过kw传入：func({‘a’: 1, ‘b’: 2})123&gt;&gt;&gt; extra = &#123;&apos;city&apos;: &apos;Beijing&apos;, &apos;job&apos;: &apos;Engineer&apos;&#125;&gt;&gt;&gt; person(&apos;Jack&apos;, 24, **extra)name: Jack age: 24 other: &#123;&apos;city&apos;: &apos;Beijing&apos;, &apos;job&apos;: &apos;Engineer&apos;&#125; 高级特征list或tuple的部分取出：切片操作取前十个：L[:10]去后十个：L[:-10]每5个取一个：L[::5] 列表生成器：生成[1x1, 2x2, 3x3, …, 10x10]‘’’[x*x for x in range(1,10)] 生成器generator循环的过程中不断推算出后续的元素，不必创建完整的list，从而节省大量的空间。创建：1.列表生成式的[]改成()，就创建了一个generator2.要把print(b)改为yield b （遇到yield输出并中断。） 迭代器Iterator可以直接作用于for循环的对象统称为可迭代对象：Iterable（list、tuple、dict、set、str、含yield）可以被next()函数调用并不断返回下一个值的对象称为迭代器：Iterator。生成器都是Iterator对象，但list、dict、str等通过使用iter()函数变成Iterator。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回下一个数据时它才会计算。 Iterator甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储全体自然数的 函数式编程匿名函数：把函数赋值给变量名，通过变量名来调用。高阶函数：函数里调用函数。 map函数:123456&gt;&gt;&gt; def f(x):... return x * x...&gt;&gt;&gt; r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9])&gt;&gt;&gt; list(r)[1, 4, 9, 16, 25, 36, 49, 64, 81] map()传入的第一个参数是f，即函数对象本身。由于结果r是一个Iterator，Iterator是惰性序列，因此通过list()函数让它把整个序列都计算出来并返回一个list。 123&gt;&gt;&gt; list(map(str, [1, 2, 3, 4, 5, 6, 7, 8, 9]))[&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;5&apos;, &apos;6&apos;, &apos;7&apos;, &apos;8&apos;, &apos;9&apos;] 把这个list所有数字转为字符串，简单实用！ reduce函数：123456&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def fn(x, y):... return x * 10 + y...&gt;&gt;&gt; reduce(fn, [1, 3, 5, 7, 9])13579 把序列[1, 3, 5, 7, 9]变换成整数13579。所以与map相比，reduce是把一个函数作用在一个序列[x1, x2, x3, …]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算。 filter函数：（过滤/筛选“掉”）12345def is_odd(n): return n % 2 == 1list(filter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15]))# 结果: [1, 5, 9, 15] 在list中，删掉偶数，只保留奇数。 sorted函数（整理）123456&gt;&gt;&gt; sorted([36, 5, -12, 9, -21])[-21, -12, 5, 9, 36]&gt;&gt;&gt; sorted([36, 5, -12, 9, -21], key=abs)[5, 9, -12, -21, 36] 排序key作用于每一个元素上 12345&gt;&gt;&gt; sorted([&apos;bob&apos;, &apos;about&apos;, &apos;Zoo&apos;, &apos;Credit&apos;], key=str.lower)[&apos;about&apos;, &apos;bob&apos;, &apos;Credit&apos;, &apos;Zoo&apos;]&gt;&gt;&gt; sorted([&apos;bob&apos;, &apos;about&apos;, &apos;Zoo&apos;, &apos;Credit&apos;], key=str.lower, reverse=True)[&apos;Zoo&apos;, &apos;Credit&apos;, &apos;bob&apos;, &apos;about&apos;] 首字母顺序排序（其实是按照ASCII的大小比较）所以代码中转换小写。倒序：reverse=True 返回函数用于闭包。 返回闭包时牢记一点：返回函数不要引用任何循环变量，或者后续会发生变化的变量。 匿名函数lambda x: x * x实际上就是： def f(x): return x * x关键字lambda表示匿名函数，冒号前面的x表示函数参数。12345678&gt;&gt;&gt; f = lambda x: x * x&gt;&gt;&gt; f&lt;function &lt;lambda&gt; at 0x101c6ef28&gt;&gt;&gt;&gt; f(5)25def build(x, y): return lambda: x * x + y * y 偏函数Python的functools模块提供了很多有用的功能，其中一个就是偏函数（Partial function）。要注意，这里的偏函数和数学意义上的偏函数不一样。123456&gt;&gt;&gt; import functools&gt;&gt;&gt; int2 = functools.partial(int, base=2)&gt;&gt;&gt; int2(&apos;1000000&apos;)64&gt;&gt;&gt; int2(&apos;1010101&apos;)85 当函数的参数个数太多，需要简化时，使用functools.partial可以创建一个新的函数，这个新函数可以固定住原函数的部分参数，从而在调用时更简单。 模块模块里面的函数名称正常的函数和变量名是公开的（public），可以被直接引用，比如：abc，x123，PI等； 类似xxx这样的变量是特殊变量，可以被直接引用，但是有特殊用途，比如上面的author，name就是特殊变量，hello模块定义的文档注释也可以用特殊变量doc访问，我们自己的变量一般不要用这种变量名； 类似_xxx和xxx这样的函数或变量就是非公开的（private），不应该被直接引用，比如_abc，abc等；1234567891011def _private_1(name): return &apos;Hello, %s&apos; % namedef _private_2(name): return &apos;Hi, %s&apos; % namedef greeting(name): if len(name) &gt; 3: return _private_1(name) else: return _private_2(name) 此模块中，公开greeting()函数，而把内部逻辑用private函数隐藏起来了，这样，调用greeting()函数不用关心内部的private函数细节，这也是一种非常有用的代码封装和抽象的方法，即： 外部不需要引用的函数全部定义成非公开的private（_xxx），只有外部需要引用的函数才定义为public。 pip安装外部模块，import 使用安装好的模块 OOP：面向对象编程]]></content>
      <tags>
        <tag>ONEDAY</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于TensorFlow学习VGG-net]]></title>
    <url>%2F2018%2F05%2F08%2F%E5%9F%BA%E4%BA%8ETensorFlow%E5%AD%A6%E4%B9%A0VGG-net%2F</url>
    <content type="text"><![CDATA[论文地址 VGG net 用来实现图片识别。固定了网络的其他参数，通过增加卷积层来增加网络深度，所有层都采用小的3*3的卷积核 架构：训练输入：为固定尺寸，224224 RGB图像。预处理：像素减去训练集的RGB均值卷积核：33，步长为1，padding。空间池化：一系列卷积后，用最大池化，2*2，步长为2.全连接层：提取特征后，接三个全连接层，前两个4096通道，第三个1000通道，最后softmax层，输出概率。每个隐藏层都使用ReLu.表格中每列代表不同网络，只是深度不同。卷积通道数量很小，第一层仅64通道，经过最大池化通道数翻倍，最后达到512通道。表格2展示每种模型参数数量。因为参数量主要集中在全连接层，尽管网络加深，权重也没大幅增加。 分类框架训练：采用mini-batch梯度下降法(小批量梯度下降)，batch size=256 采用动量优化算法(模拟惯性，更新时候在一定程度上保留之前更新方向，同时利用当前batch的梯度微调最终更新方向，增加稳定性，拜托局部最优)，momentum=0.9； 采用L2正则化，惩罚系数0.00005,；doupout比率为0.5 初始学习率为0.001,后期衰减为原来的0.01,再0.1，再1 迭代次数370K（74epochs）（当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个 epoch）； 数据增强采：用随机裁剪，水平翻转，RGB颜色变化； 设置图片大小的方法：定义S代表经过各向同性缩放的训练图像最小边（？）第一种方法，针对单尺寸图像训练，S=256或384，把输入图片随机裁剪224*225大小图片，原则上S可取任意不小于224的值。第二种方法是多尺度训练。每张图从【S min，S max】中随机选取S进行尺寸缩放，由于目标尺寸不定，这种方法有效。 测试：（全卷积，不裁剪）对于已经训练好的卷积网络的一张输入图片，用下面的方法分类：first，图像的最小边被各向同性的缩放到预定尺寸Q；second，将原来的全连接层改为卷积层，在未被裁剪的的全图像上运用卷积网络，输出是一个与输入尺寸相关的分类得分图，输出通道与类别数相同。last，对分类得分图进行空间平均化，得到固定尺寸的分类得分向量。数据增强：水平翻转图片，最终取原始图片和翻转图片的softmax分类概率平均值作为最终得分。 最近常用tensorflow函数总结:x = tf.placeholder(tf.float32,shape=[1,224,224,3])占位，BATCH_SIZE为1，图片大小为224*224像素，3通道彩图。sess.run（求分类评估值的节点，feed_dict{x: }） np.load(“名.npy”).item()np.save(“名.npy”,某数组)将数组按照二进制的形式进行读入，写出。扩展名为：npy。其中，item（）表示遍历每个元素。 tf.shape(a)返回a的维度 tf.split(切谁，怎么切，哪个维度)tf.concat(整合谁，哪个维度) 更多：https://tensorflow.google.cn/使用vgg16实现图片识别：图1图2图3 代码解析： vgg16.py：还原网络和参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115#!/usr/bin/python#coding:utf-8import inspect #检查运行信息的模块import osimport numpy as npimport tensorflow as tfimport timeimport matplotlib.pyplot as pltVGG_MEAN = [103.939, 116.779, 123.68] #样本rgb平均值class Vgg16(): def __init__(self, vgg16_path=None): if vgg16_path is None: vgg16_path = os.path.join(os.getcwd(), &quot;vgg16.npy&quot;)#加入路径 ，os.getcwd()用于返回当前工作目录 self.data_dict = np.load(vgg16_path, encoding=&apos;latin1&apos;).item()#遍历，模型参数读入字典 def forward(self, images): print(&quot;build model started&quot;) start_time = time.time() #获取前项传播的开始时间 rgb_scaled = images * 255.0 #逐像素乘上255.0 #从GRB转换色道，到BGR red, green, blue = tf.split(rgb_scaled,3,3) bgr = tf.concat([ blue - VGG_MEAN[0], green - VGG_MEAN[1], red - VGG_MEAN[2]],3) #逐像素减去像素平均值，可以移除平均亮度，常用于灰度图像上 #接下来构建VGG的16层网络（包括5段卷积，3层全连接）并逐层根据网络空间读取网络参数 #第一段卷积，有两个卷积层，加最大池化，用来缩小图片尺寸 self.conv1_1 = self.conv_layer(bgr, &quot;conv1_1&quot;) #传入name，获取卷积核和偏置，并卷积运算，经过激活函数后返回。 self.conv1_2 = self.conv_layer(self.conv1_1, &quot;conv1_2&quot;) #池化 self.pool1 = self.max_pool_2x2(self.conv1_2, &quot;pool1&quot;) #第二段卷积，和第一段相同 self.conv2_1 = self.conv_layer(self.pool1, &quot;conv2_1&quot;) self.conv2_2 = self.conv_layer(self.conv2_1, &quot;conv2_2&quot;) self.pool2 = self.max_pool_2x2(self.conv2_2, &quot;pool2&quot;) #第三段卷积，三个卷积层，一个最大池化 self.conv3_1 = self.conv_layer(self.pool2, &quot;conv3_1&quot;) self.conv3_2 = self.conv_layer(self.conv3_1, &quot;conv3_2&quot;) self.conv3_3 = self.conv_layer(self.conv3_2, &quot;conv3_3&quot;) self.pool3 = self.max_pool_2x2(self.conv3_3, &quot;pool3&quot;) #第四段卷积，三个卷积层，一个最大池化 self.conv4_1 = self.conv_layer(self.pool3, &quot;conv4_1&quot;) self.conv4_2 = self.conv_layer(self.conv4_1, &quot;conv4_2&quot;) self.conv4_3 = self.conv_layer(self.conv4_2, &quot;conv4_3&quot;) self.pool4 = self.max_pool_2x2(self.conv4_3, &quot;pool4&quot;) #第五段卷积，三个卷积层，一个最大池化 self.conv5_1 = self.conv_layer(self.pool4, &quot;conv5_1&quot;) self.conv5_2 = self.conv_layer(self.conv5_1, &quot;conv5_2&quot;) self.conv5_3 = self.conv_layer(self.conv5_2, &quot;conv5_3&quot;) self.pool5 = self.max_pool_2x2(self.conv5_3, &quot;pool5&quot;) #第六段全连接层 self.fc6 = self.fc_layer(self.pool5, &quot;fc6&quot;)#根据命名空间fc6，做加权求和 self.relu6 = tf.nn.relu(self.fc6) #激活 #第七段全连接层 self.fc7 = self.fc_layer(self.relu6, &quot;fc7&quot;) self.relu7 = tf.nn.relu(self.fc7) #第八层全连接 self.fc8 = self.fc_layer(self.relu7, &quot;fc8&quot;) self.prob = tf.nn.softmax(self.fc8, name=&quot;prob&quot;)#softmax分类，得到属于各个类别的概率。 end_time = time.time() #获取结束时间 print((&quot;time consuming: %f&quot; % (end_time-start_time)))#耗时 self.data_dict = None #清空本次读到地模型字典 #卷积计算的相关定义， def conv_layer(self, x, name): with tf.variable_scope(name):#根据命名空间找到网络参数。 w = self.get_conv_filter(name) #读取卷积核 conv = tf.nn.conv2d(x, w, [1, 1, 1, 1], padding=&apos;SAME&apos;)卷积计算 conv_biases = self.get_bias(name) #读取偏置 result = tf.nn.relu(tf.nn.bias_add(conv, conv_biases)) #加上偏置，并激活 return result def get_conv_filter(self, name):#其中获取卷积核的定义 return tf.constant(self.data_dict[name][0], name=&quot;filter&quot;) def get_bias(self, name):#其中获取偏置的定义 return tf.constant(self.data_dict[name][1], name=&quot;biases&quot;) def max_pool_2x2(self, x, name):#其中最大池的定义 return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;, name=name) #定义全连接层的前项传播计算 def fc_layer(self, x, name): with tf.variable_scope(name):#根据命名空间做计算 shape = x.get_shape().as_list() #该层维度信息 dim = 1 for i in shape[1:]: dim *= i #每层的维度相乘 x = tf.reshape(x, [-1, dim])#改变特征图形状，多维度特征的拉伸操作，只第六层用 w = self.get_fc_weight(name) #读取w b = self.get_bias(name) #读取b result = tf.nn.bias_add(tf.matmul(x, w), b)#加权求和加偏置 return result #定义获取权重 def get_fc_weight(self, name): return tf.constant(self.data_dict[name][0], name=&quot;weights&quot;) utils：辅助函数，包括读取输入图片，计算百分比形势的概率1234567891011121314151617181920212223242526272829303132333435363738394041#!/usr/bin/python#coding:utf-8from skimage import io, transformimport numpy as npimport matplotlib.pyplot as pltimport tensorflow as tffrom pylab import mplmpl.rcParams[&apos;font.sans-serif&apos;]=[&apos;SimHei&apos;] # 正常显示中文标签mpl.rcParams[&apos;axes.unicode_minus&apos;]=False # 正常显示正负号def load_image(path): fig = plt.figure(&quot;Centre and Resize&quot;) img = io.imread(path) #读取 img = img / 255.0 #归一化 ax0 = fig.add_subplot(131) ax0.set_xlabel(u&apos;Original Picture&apos;) ax0.imshow(img) short_edge = min(img.shape[:2]) y = (img.shape[0] - short_edge) / 2 x = (img.shape[1] - short_edge) / 2 crop_img = img[y:y+short_edge, x:x+short_edge] ax1 = fig.add_subplot(132) ax1.set_xlabel(u&quot;Centre Picture&quot;) ax1.imshow(crop_img) re_img = transform.resize(crop_img, (224, 224)) ax2 = fig.add_subplot(133) ax2.set_xlabel(u&quot;Resize Picture&quot;) ax2.imshow(re_img) img_ready = re_img.reshape((1, 224, 224, 3)) return img_readydef percent(value): return &apos;%.2f%%&apos; % (value * 100) app.py:123456789101112131415161718192021222324252627282930313233343536#coding:utf-8import numpy as npimport tensorflow as tfimport matplotlib.pyplot as pltimport vgg16import utilsfrom Nclasses import labelsimg_path = raw_input(&apos;Input the path and image name:&apos;)img_ready = utils.load_image(img_path) #调用并预处理图片fig=plt.figure(u&quot;Top-5 预测结果&quot;) with tf.Session() as sess: images = tf.placeholder(tf.float32, [1, 224, 224, 3])#占位，类型维度 vgg = vgg16.Vgg16() vgg.forward(images) probability = sess.run(vgg.prob, feed_dict=&#123;images:img_ready&#125;) top5 = np.argsort(probability[0])[-1:-6:-1] print &quot;top5:&quot;,top5 values = [] bar_label = [] for n, i in enumerate(top5): print &quot;n:&quot;,n print &quot;i:&quot;,i values.append(probability[0][i]) bar_label.append(labels[i]) print i, &quot;:&quot;, labels[i], &quot;----&quot;, utils.percent(probability[0][i]) ax = fig.add_subplot(111) ax.bar(range(len(values)), values, tick_label=bar_label, width=0.5, fc=&apos;g&apos;) ax.set_ylabel(u&apos;probabilityit&apos;) ax.set_title(u&apos;Top-5&apos;) for a,b in zip(range(len(values)), values): ax.text(a, b+0.0005, utils.percent(b), ha=&apos;center&apos;, va = &apos;bottom&apos;, fontsize=7) plt.show() 还有两个文件：1：vgg16.npy：保存了网络参数2：Nclasses.py:保存了编号和物体名称的字典]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DCGAN的原理及应用]]></title>
    <url>%2F2018%2F05%2F06%2FDCGAN%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文研究基于GAN基础上，结合卷积神经网络（CNN）的DCGAN模型。相关论文：Unsupervised Representations Learning With Deep Convolutional Generative Adversarial Networks DCGAN简介DCGAN是将CNN与GAN的一种结合。其将卷积网络引入到生成式模型当中来做无监督的训练，利用卷积网络强大的特征提取能力来提高生成网络的学习效果。DCGAN的原理和GAN对抗生成是一样的。它只是把GAN的G和D换成了两个卷积神经网络（CNN）。但不是直接换就可以了，DCGAN对卷积神经网络的结构做了一些改变，以提高样本的质量和收敛的速度，这些改变有： 取消所有pooling层。G网络中使用转置卷积（transposed convolutional layer）进行上采样，D网络中用加入stride的卷积代替pooling。 除了生成器模型的输出层和判别器模型的输入层，在网络其它层上都使用了Batch Normalization，使用BN可以稳定学习，有助于处理初始化不良导致的训练问题。 去掉全连接层，使网络变为全卷积网络 G网络中使用ReLU作为激活函数，最后一层使用tanh D网络中使用LeakyReLU作为激活函数 其中，转置卷积（也称反卷积）transposed conv或者deconv。动图一：卷积动图二：转置卷积上图为卷积与转置卷积，注意图中蓝色（下面）是输入，绿色（上面）是输出，卷积和反卷积在 p、s、k 等参数一样时，是相当于 i 和 o 调了个位。 这里说明了反卷积的时候，是有补0的，即使人家管这叫no padding。图中反卷积应该从蓝色 2×2 扩展成绿色 4×4。转置并不是指这个 3×3 的核 w 变为 wT，但如果将卷积计算写成矩阵乘法（在程序中，为了提高卷积操作的效率，就可以这么干，比如tensorflow中就是这种实现），而这样的矩阵乘法，恰恰等于 w 左右翻转再上下翻转后与补0的 Y 卷积的情况。 其中，batch normalization相关：传统的神经网络，只是在将样本xx输入输入层之前对xx进行标准化处理（减均值，除标准差），以降低样本间的差异性。BN是在此基础上，不仅仅只对输入层的输入数据x进行标准化，还对每个隐藏层的输入进行标准化。标准化后的x乘以权值矩阵Wh1加上偏置bh1得到第一层的输入wh1x+bh1,经过激活函数得到h1=ReLU(wh1x+bh1)，然而加入BN后, h1的计算流程如虚线框所示：再其中，求平均数mean, variance = tf.nn.moments(x, axes, name=None, keep_dims=False)这个函数的输入参数x表示样本，形如[batchsize, height, width, kernels]axes表示在哪个维度上求解 。函数输出均值和方差。移动平均就是滑动平均的方法，详见另一篇文章：使用tensorflow实现cnn。 生成网络模型示意图： 通过TensorFlow实现DCGANDCGAN源码解析：参考博客源码地址先看main.py：12345678910111213141516with tf.Session(config=run_config) as sess: if FLAGS.dataset == &apos;mnist&apos;: dcgan = DCGAN( sess, input_width=FLAGS.input_width, input_height=FLAGS.input_height, output_width=FLAGS.output_width, output_height=FLAGS.output_height, batch_size=FLAGS.batch_size, y_dim=10, c_dim=1, dataset_name=FLAGS.dataset, input_fname_pattern=FLAGS.input_fname_pattern, is_crop=FLAGS.is_crop, checkpoint_dir=FLAGS.checkpoint_dir, sample_dir=FLAGS.sample_dir) 因为我们使用DCGAN来生成MNIST数字手写体图像，注意这里的y_dim=10，表示0到9这10个类别，c_dim=1，表示灰度图像。再看model.py：DCGAN-tensorflow核心是model.pymodel.py定义了生成器和判别器，其中生成器使用deconv2d,判别器使用conv2d,这里conv2d是通过调用tensorflown的conv2d实现和权重相乘，再用bias_add实现偏置项相加。123456789101112131415161718192021def discriminator(self, image, y=None, reuse=False): with tf.variable_scope(&quot;discriminator&quot;) as scope: if reuse: scope.reuse_variables() yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) x = conv_cond_concat(image, yb) h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name=&apos;d_h0_conv&apos;)) h0 = conv_cond_concat(h0, yb) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim, name=&apos;d_h1_conv&apos;))) h1 = tf.reshape(h1, [self.batch_size, -1]) h1 = tf.concat_v2([h1, y], 1) h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, &apos;d_h2_lin&apos;))) h2 = tf.concat_v2([h2, y], 1) h3 = linear(h2, 1, &apos;d_h3_lin&apos;) return tf.nn.sigmoid(h3), h3 123456789101112def conv2d(input_, output_dim, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=&quot;conv2d&quot;):with tf.variable_scope(name): w = tf.get_variable(&apos;w&apos;, [k_h, k_w, input_.get_shape()[-1], output_dim], initializer=tf.truncated_normal_initializer(stddev=stddev)) conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=&apos;SAME&apos;) biases = tf.get_variable(&apos;biases&apos;, [output_dim], initializer=tf.constant_initializer(0.0)) conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape()) return conv 这里batch_size=64，image的维度为[64 28 28 1]，y的维度是[64 10]，yb的维度[64 1 1 10]，x将image和yb连接起来，这相当于是使用了Conditional GAN，为图像提供标签作为条件信息，于是x的维度是[64 28 28 11]，将x输入到卷积层conv2d，conv2d的代码如下：123456789101112def conv2d(input_, output_dim, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=&quot;conv2d&quot;): with tf.variable_scope(name): w = tf.get_variable(&apos;w&apos;, [k_h, k_w, input_.get_shape()[-1], output_dim], initializer=tf.truncated_normal_initializer(stddev=stddev)) conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding=&apos;SAME&apos;) biases = tf.get_variable(&apos;biases&apos;, [output_dim], initializer=tf.constant_initializer(0.0)) conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape()) return conv 卷积核的大小为55，stride为[1 2 2 1]，通过2的卷积步长可以替代pooling进行降维，padding=‘SAME’，则卷积的输出维度为[64 14 14 11]。然后使用batch normalization及leaky ReLU的激活层，输出与yb再进行concat，得到h0，维度为[64 14 14 21]。同理，h1的维度为[64 77*74+10]，h2的维度为[64 1024+10]，然后连接一个线性输出，得到h3，维度为[64 1]，由于我们希望判别器的输出代表概率，所以最终使用一个sigmoid的激活。123456789101112131415161718192021222324252627def generator(self, z, y=None): with tf.variable_scope(&quot;generator&quot;) as scope: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.expand_dims(tf.expand_dims(y, 1),2) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = tf.concat_v2([z, y], 1) h0 = tf.nn.relu( self.g_bn0(linear(z, self.gfc_dim, &apos;g_h0_lin&apos;))) h0 = tf.concat_v2([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, &apos;g_h1_lin&apos;))) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2(deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name=&apos;g_h2&apos;))) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid( deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name=&apos;g_h3&apos;)) output_height和output_width为28，因此s_h和s_w为28，s_h2和s_w2为14，s_h4和s_w4为7。在这里z为平均分布的随机分布数，维度为[64 100]，y的维度为[64 10]，yb的维度是[64 1 1 10]，z与y进行一个concat得到[64 110]的tensor，输入到一个线性层，输出维度是[64 1024]，再经过batch normalization以及ReLU激活，并与y进行concat，输出h0的维度是[64 1034]，同样的再经过一个线性层输出维度为[64 12877]，再进行reshape并与yb进行concat，得到h1，维度为[64 7 7 138]，然后输入到一个deconv2d，做一个反卷积，也就是文中说的fractional strided convolutions，再经过batch normalization以及ReLU激活，并与yb进行concat，输出h2的维度是[64 14 14 138]，最后再输入到deconv2d层以及sigmoid激活，得到生成器的输出，维度为[64 28 28 1]。生成器以及判别器的输出：123456self.G = self.generator(self.z, self.y)self.D, self.D_logits = \ self.discriminator(inputs, self.y, reuse=False)self.D_, self.D_logits_ = \ self.discriminator(self.G, self.y, reuse=True) 其中D表示真实数据的判别器输出，D_表示生成数据的判别器输出。再看损失函数：123456789self.d_loss_real = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits, targets=tf.ones_like(self.D)))self.d_loss_fake = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits_, targets=tf.zeros_like(self.D_)))self.g_loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits_, targets=tf.ones_like(self.D_))) 即对于真实数据，判别器的损失函数d_loss_real为判别器输出与1的交叉熵，而对于生成数据，判别器的损失函数d_loss_fake为输出与0的交叉熵，因此判别器的损失函数d_loss=d_loss_real+d_loss_fake；生成器的损失函数是g_loss判别器对于生成数据的输出与1的交叉熵。优化器：1234d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \ .minimize(self.d_loss, var_list=self.d_vars)g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \ .minimize(self.g_loss, var_list=self.g_vars) 训练：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849for epoch in xrange(config.epoch): batch_idxs = min(len(data_X), config.train_size) // config.batch_size for idx in xrange(0, batch_idxs): batch_images = data_X[idx*config.batch_size:(idx+1)*config.batch_size] batch_labels = data_y[idx*config.batch_size:(idx+1)*config.batch_size] batch_images = np.array(batch).astype(np.float32)[:, :, :, None] batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \ .astype(np.float32) # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict=&#123; self.inputs: batch_images, self.z: batch_z, self.y:batch_labels, &#125;) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict=&#123; self.z: batch_z, self.y:batch_labels, &#125;) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict=&#123; self.z: batch_z, self.y:batch_labels &#125;) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval(&#123; self.z: batch_z, self.y:batch_labels &#125;) errD_real = self.d_loss_real.eval(&#123; self.inputs: batch_images, self.y:batch_labels &#125;) errG = self.g_loss.eval(&#123; self.z: batch_z, self.y: batch_labels &#125;) counter += 1]]></content>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GAN学习报告]]></title>
    <url>%2F2018%2F04%2F29%2FGAN%E5%AD%A6%E4%B9%A0%E6%8A%A5%E5%91%8A%2F</url>
    <content type="text"><![CDATA[GAN 启发自博弈论中的二人零和博弈（two-player game），论文：Generative Adversarial Networks（arxiv：https://arxiv.org/abs/1406.2661）GAN 模型中的两位博弈方分别由生成式模型G（generative model）和判别式模型N（discriminative model）组成。 基本原理 G是一个生成图片的网络，它接收一个随机的噪声z，通过这个噪声生成图片，记做G(z)。D是一个判别网络，判别一张图片是不是“真实的”。它的输入参数是x，x代表一张图片，输出D（x）代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片。 在训练过程中，生成网络G的目标就是尽量生成真实的图片去欺骗判别网络D。而D的目标就是尽量把G生成的图片和真实的图片分别开来。这样，G和D构成了一个动态的“博弈过程”。 最后对抗网网络的目标：对于输入一个随机噪声z，生成网络G生成的假样本G(z)进去了判别网络以后，判别网络D给出的结果D（G(z)）是一个接近 0.5 的值，也就是说无法分辨是真（1）还是假（0），达到了平衡。从而我们的G可以生成以假乱真的图片。 模型判别模型，直观来看就是一个简单的神经网络结构，输入就是一副图像，输出就是一个概率值，用于判断真假使用（概率值大于 0.5 那就是真，小于 0.5 那就是假）。生成模型，生成模型同样也可以看成是一个神经网络模型，输入是一组随机噪声z，输出是一个仿样本G（z）。 实现方法 生成模型与对抗模型可以说是完全独立的两个模型 但是训练时让他们：单独交替迭代训练 先用真实数据集和生成模型产生的数据集进行标签，来训练判别模型。 而训练生成模型时，把生成数据标签成真实数据，让判别模型去判别。（注意，不更新判别模型里面的参数）把误差一直传，传到生成网络那块后更新生成网络的参数。 重复上述过程。 简单来说生成器的结果会有两种应用，一种是标明为假，和真实数据混合起来为判别模型。一种把自己标注为真，用当前的判别模型来进行训练 目标公式 公式详解在下图 整个式子由两部分构成。x表示真实图片，z表示输入G网络的噪声，而G(z)表示G网络生成的图片。D(x)表示D网络判断真实图片是否真实的概率（因为x就是真实的，所以对于D来说，这个值越接近1越好）。而D(G(z))是D网络判断G生成的图片的是否真实的概率。 单独优化D时，想要让真实数据x的输出让D（x）越大越好，让生成数据D(G(z))越小越好。公式中将D(G(z))写为1-D(G(z))是可以让整个公式统一，V（D）越大越好。 单独优化G时，只和公式第二部分相关，其目标就是让D(G(z)越大越好，为了整合统一写为1-D(G(z)，就是V（G）越小越好。 用tensorflow实现简单的GANabout tensorflow：tensorflow过程主要分为前向传播和反向传播。 前向传播搭建网络结构。 反向传播训练网络参数。 本次目的：实现通过mnist数据集，训练得到生成数字图像 每训练1000 次保存16张生成图片 共训练500000轮。 参考代码：https://github.com/liuzheng007/GAN/blob/master/gan.py KL散度又称相对熵,用来衡量两个分布的差异。KL 散度有很多有用的性质，最重要的是它是非负的。KL 散度为 0时，是 当且仅当 P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是 『几乎处处』 相同的。因为 KL 散度是非负的并且衡量的是两个分布之间的差异，它经常被用作分布之间的某种距离。然而，它并不是真的距离因为它不是对称的。 最优判别器在上面极小极大博弈的第一步中，给定生成器 G，最大化 V(D,G) 而得出最优判别器 D。其中，最大化 V(D,G) 评估了 P（G（z）） 和 P（x） 之间的差异或距离。因为在原论文中价值函数可写为在 x 上的积分，即将数学期望展开为积分形式：其实求积分的最大值可以转化为求被积函数的最大值。而求被积函数的最大值是为了求得最优判别器 D，因此不涉及判别器的项都可以看作为常数项。如图，因此被积函数可表示为 aD(x)+blog(1-D(x))【 a,b∈(0,1)】 令判别器 D(x) 等于 y，那么被积函数可以写为：为了找到最优的极值点，如果 a+b≠0，我们可以对y求一阶导，令导数为零。继续求表达式 f(y) 在该驻点的二阶导，其中 a,b∈(0,1)。因为一阶导等于零、二阶导小于零，所以我们知道 a/(a+b) 为极大值。将 a=P(x)、b=P（G(x)） 代入该极值，最优判别器 D(x)=P（x）/(P(x)+P（G(x))），此时价值函数 V(G,D) 取极大值。其实该最优的 D 在实践中并不是可计算的，但在数学上十分重要。我们并不知道先验的 P(x)，所以我们在训练中永远不会用到它。另一方面，它的存在令我们可以证明最优的 G 是存在的，并且在训练中我们只需要逼近 D（x）。 代码解析：生成模型首先需要定义一个生成器 G，该生成器需要将输入的随机噪声变换为图像。以下是定义的生成模型，该模型首先输入有 100 个元素的向量，该向量随机生成于某分布。随后利用两个全连接层接连将该输入向量扩展到 1024 维和 12877 维，后面就开始将全连接层所产生的一维张量重新塑造成二维张量，即 MNIST 中的灰度图。我们注意到该模型采用的激活函数为 tanh，所以也尝试过将其转换为 relu 函数，但发现生成模型如果转化为 relu 函数，那么它的输出就会成为一片灰色。 由全连接传递的数据会经过几个上采样层和卷积层，我们注意到最后一个卷积层所采用的卷积核为 1，所以经过最后卷积层所生成的图像是一张二维灰度图像，123456789101112131415161718192021222324def generator_model(): #下面搭建生成器的架构，首先导入序贯模型（sequential），即多个网络层的线性堆叠 model = Sequential() #添加一个全连接层，输入为100维向量，输出为1024维 model.add(Dense(input_dim=100, output_dim=1024)) #添加一个激活函数tanh model.add(Activation(&apos;tanh&apos;)) #添加一个全连接层，输出为128×7×7维度 model.add(Dense(128*7*7)) #添加一个批量归一化层，该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1 model.add(BatchNormalization()) model.add(Activation(&apos;tanh&apos;)) #Reshape层用来将输入shape转换为特定的shape，将含有128*7*7个元素的向量转化为7×7×128张量 model.add(Reshape((7, 7, 128), input_shape=(128*7*7,))) #2维上采样层，即将数据的行和列分别重复2次 model.add(UpSampling2D(size=(2, 2))) #添加一个2维卷积层，卷积核大小为5×5，激活函数为tanh，共64个卷积核，并采用padding以保持图像尺寸不变 model.add(Conv2D(64, (5, 5), padding=&apos;same&apos;)) model.add(Activation(&apos;tanh&apos;)) model.add(UpSampling2D(size=(2, 2))) #卷积核设为1即输出图像的维度 model.add(Conv2D(1, (5, 5), padding=&apos;same&apos;)) model.add(Activation(&apos;tanh&apos;)) return model 拼接前面定义的是可生成图像的模型 G(z;θ_g)，而我们在训练生成模型时，需要固定判别模型 D 以极小化价值函数而寻求更好的生成模型，这就意味着我们需要将生成模型与判别模型拼接在一起，并固定 D 的权重以训练 G 的权重。下面就定义了这一过程，我们先添加前面定义的生成模型，再将定义的判别模型拼接在生成模型下方，并且我们将判别模型设置为不可训练。因此，训练这个组合模型才能真正更新生成模型的参数。123456789def generator_containing_discriminator(g, d): #将前面定义的生成器架构和判别器架构组拼接成一个大的神经网络，用于判别生成的图片 model = Sequential() #先添加生成器架构，再令d不可训练，即固定d #因此在给定d的情况下训练生成器，即通过将生成的结果投入到判别器进行辨别而优化生成器 model.add(g) d.trainable = False model.add(d) return model 判别模型判别模型相对来说就是比较传统的图像识别模型，前面我们可以按照经典的方法采用几个卷积层与最大池化层，而后再展开为一维张量并采用几个全连接层作为架构。12345678910111213141516171819202122232425def discriminator_model(): #下面搭建判别器架构，同样采用序贯模型 model = Sequential() #添加2维卷积层，卷积核大小为5×5，激活函数为tanh，输入shape在‘channels_first’模式下为（samples,channels，rows，cols） #在‘channels_last’模式下为（samples,rows,cols,channels），输出为64维 model.add( Conv2D(64, (5, 5), padding=&apos;same&apos;, input_shape=(28, 28, 1)) ) model.add(Activation(&apos;tanh&apos;)) #为空域信号施加最大值池化，pool_size取（2，2）代表使图片在两个维度上均变为原长的一半 model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(128, (5, 5))) model.add(Activation(&apos;tanh&apos;)) model.add(MaxPooling2D(pool_size=(2, 2))) #Flatten层把多维输入一维化，常用在从卷积层到全连接层的过渡 model.add(Flatten()) model.add(Dense(1024)) model.add(Activation(&apos;tanh&apos;)) #一个结点进行二值分类，并采用sigmoid函数的输出作为概念 model.add(Dense(1)) model.add(Activation(&apos;sigmoid&apos;)) return model 训练训练过程可简述为： 加载 MNIST 数据将数据分割为训练与测试集，并赋值给变量设置训练模型的超参数编译模型的训练过程在每一次迭代内，抽取生成图像与真实图像，并打上标注随后将数据投入到判别模型中，并进行训练与计算损失固定判别模型，训练生成模型并计算损失，结束这一次迭代1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283def train(BATCH_SIZE): (X_train, y_train), (X_test, y_test) = mnist.load_data() #iamge_data_format选择&quot;channels_last&quot;或&quot;channels_first&quot;，该选项指定了Keras将要使用的维度顺序。 #&quot;channels_first&quot;假定2D数据的维度顺序为(channels, rows, cols)，3D数据的维度顺序为(channels, conv_dim1, conv_dim2, conv_dim3) #转换字段类型，并将数据导入变量中 X_train = (X_train.astype(np.float32) - 127.5)/127.5 X_train = X_train[:, :, :, None] X_test = X_test[:, :, :, None] # X_train = X_train.reshape((X_train.shape, 1) + X_train.shape[1:]) #将定义好的模型架构赋值给特定的变量 d = discriminator_model() g = generator_model() d_on_g = generator_containing_discriminator(g, d) #定义生成器模型判别器模型更新所使用的优化算法及超参数 d_optim = SGD(lr=0.001, momentum=0.9, nesterov=True) g_optim = SGD(lr=0.001, momentum=0.9, nesterov=True) #编译三个神经网络并设置损失函数和优化算法，其中损失函数都是用的是二元分类交叉熵函数。编译是用来配置模型学习过程的 g.compile(loss=&apos;binary_crossentropy&apos;, optimizer=&quot;SGD&quot;) d_on_g.compile(loss=&apos;binary_crossentropy&apos;, optimizer=g_optim) #前一个架构在固定判别器的情况下训练了生成器，所以在训练判别器之前先要设定其为可训练。 d.trainable = True d.compile(loss=&apos;binary_crossentropy&apos;, optimizer=d_optim) #下面在满足epoch条件下进行训练 for epoch in range(30): print(&quot;Epoch is&quot;, epoch) #计算一个epoch所需要的迭代数量，即训练样本数除批量大小数的值取整；其中shape[0]就是读取矩阵第一维度的长度 print(&quot;Number of batches&quot;, int(X_train.shape[0]/BATCH_SIZE)) #在一个epoch内进行迭代训练 for index in range(int(X_train.shape[0]/BATCH_SIZE)): #随机生成的噪声服从均匀分布，且采样下界为-1、采样上界为1，输出BATCH_SIZE×100个样本；即抽取一个批量的随机样本 noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100)) #抽取一个批量的真实图片 image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE] #生成的图片使用生成器对随机噪声进行推断；verbose为日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录 generated_images = g.predict(noise, verbose=0) #每经过100次迭代输出一张生成的图片 if index % 100 == 0: image = combine_images(generated_images) image = image*127.5+127.5 Image.fromarray(image.astype(np.uint8)).save( &quot;./GAN/&quot;+str(epoch)+&quot;_&quot;+str(index)+&quot;.png&quot;) #将真实的图片和生成的图片以多维数组的形式拼接在一起，真实图片在上，生成图片在下 X = np.concatenate((image_batch, generated_images)) #生成图片真假标签，即一个包含两倍批量大小的列表；前一个批量大小都是1，代表真实图片，后一个批量大小都是0，代表伪造图片 y = [1] * BATCH_SIZE + [0] * BATCH_SIZE #判别器的损失；在一个batch的数据上进行一次参数更新 d_loss = d.train_on_batch(X, y) print(&quot;batch %d d_loss : %f&quot; % (index, d_loss)) #随机生成的噪声服从均匀分布 noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100)) #固定判别器 d.trainable = False #计算生成器损失；在一个batch的数据上进行一次参数更新 g_loss = d_on_g.train_on_batch(noise, [1] * BATCH_SIZE) #令判别器可训练 d.trainable = True print(&quot;batch %d g_loss : %f&quot; % (index, g_loss)) #每100次迭代保存一次生成器和判别器的权重 if index % 100 == 9: g.save_weights(&apos;generator&apos;, True) d.save_weights(&apos;discriminator&apos;, True)]]></content>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用tensorflow实现cnn]]></title>
    <url>%2F2018%2F04%2F24%2F%E4%BD%BF%E7%94%A8tensorflow%E5%AE%9E%E7%8E%B0cnn%2F</url>
    <content type="text"><![CDATA[cnn简单来说就是把原始图片进行特征提取后再来进入全连接网络。具体的特征提取方法：卷积：tf.nn.conv2d()池化：poll=tf.nn.max_pool(), tf.nn.avg_pool()舍弃： tf.nn.dropout() 其中池化分为最大池化和平均池化。最大池化提取纹理。均值池化保留背景特征。 使用全零填充（padding）padding=‘SAME’如果不用：padding=’VALID’ lenet5代码拆解分析前向传播中： #coding:utf-8 import tensorflow as tf IMAGE_SIZE = 28 #28*28图片 NUM_CHANNELS = 1 #灰度图 CONV1_SIZE = 5 #第一层核的大小是5 CONV1_KERNEL_NUM = 32#第一层32个核 CONV2_SIZE = 5 #第二层核的大小是5 CONV2_KERNEL_NUM = 64#第二层64个核 FC_SIZE = 512 #第一层全连接网络神经元512个 OUTPUT_NODE = 10 #第二层全连接网络神经元10个，对应十分类的输出 def get_weight(shape, regularizer):#定义了权重生成函数 w = tf.Variable(tf.truncated_normal(shape,stddev=0.1))#生成去掉过大偏离点的正态分布随机数 if regularizer != None: tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w)) return w def get_bias(shape): #定义了偏置b生成函数 b = tf.Variable(tf.zeros(shape)) return b def conv2d(x,w): #定义卷积计算函数 #x是四阶输入（batch，行列分辨率，输入的通道数）， #w是卷积核描述的四阶张量（行列分辨率，通道数，核个数）， #strides 是滑动步长，第一四阶固定为“1”，二三阶为横纵滑动步长, #使用零填充 return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME') def max_pool_2x2(x): #定义最大池化 #x是四阶输入（batch，行列分辨率，输入的通道数） #池化核大小为2*2 #滑动步长，行列都为2 #使用零填充 return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') def forward(x, train, regularizer):#前向传播网络结构 conv1_w = get_weight([CONV1_SIZE, CONV1_SIZE, NUM_CHANNELS, CONV1_KERNEL_NUM], regularizer) #初始化第一层卷积核 conv1_b = get_bias([CONV1_KERNEL_NUM]) #初始化第一层偏置 conv1 = conv2d(x, conv1_w) #执行卷积计算 relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_b)) #加偏置，使用激活函数 pool1 = max_pool_2x2(relu1) #最大池化 conv2_w = get_weight([CONV2_SIZE, CONV2_SIZE, CONV1_KERNEL_NUM, CONV2_KERNEL_NUM],regularizer) conv2_b = get_bias([CONV2_KERNEL_NUM]) conv2 = conv2d(pool1, conv2_w) relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_b)) pool2 = max_pool_2x2(relu2) pool_shape = pool2.get_shape().as_list()#将pool2输出矩阵的维度存入list nodes = pool_shape[1] * pool_shape[2] * pool_shape[3] #提取list 长，宽，深度，再进行相乘得到所有特征点个数 reshaped = tf.reshape(pool2, [pool_shape[0], nodes]) #将pool2变形为 batch行,特征点个数这么多列的二维形状。用来喂入全连接网络 fc1_w = get_weight([nodes, FC_SIZE], regularizer) fc1_b = get_bias([FC_SIZE]) fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_w) + fc1_b) #全连接网络 if train: fc1 = tf.nn.dropout(fc1, 0.5)#训练阶段使用50%的dropout。 fc2_w = get_weight([FC_SIZE, OUTPUT_NODE], regularizer) fc2_b = get_bias([OUTPUT_NODE]) y = tf.matmul(fc1, fc2_w) + fc2_b#第二层全连接网络。 return y 反向传播中：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#coding:utf-8import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataimport mnist_lenet5_forwardimport osimport numpy as npBATCH_SIZE = 100LEARNING_RATE_BASE = 0.005 LEARNING_RATE_DECAY = 0.99 REGULARIZER = 0.0001 STEPS = 50000 MOVING_AVERAGE_DECAY = 0.99 MODEL_SAVE_PATH=&quot;./model/&quot; MODEL_NAME=&quot;mnist_model&quot; def backward(mnist): #形状调整：batch size，行列分辨率，通道数 x = tf.placeholder(tf.float32,[ BATCH_SIZE, mnist_lenet5_forward.IMAGE_SIZE, mnist_lenet5_forward.IMAGE_SIZE, mnist_lenet5_forward.NUM_CHANNELS]) y_ = tf.placeholder(tf.float32, [None, mnist_lenet5_forward.OUTPUT_NODE]) y = mnist_lenet5_forward.forward(x,True, REGULARIZER) #训练时使用dropout global_step = tf.Variable(0, trainable=False) ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1)) cem = tf.reduce_mean(ce) loss = cem + tf.add_n(tf.get_collection(&apos;losses&apos;)) learning_rate = tf.train.exponential_decay( LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY, staircase=True) train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step) ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step) ema_op = ema.apply(tf.trainable_variables()) with tf.control_dependencies([train_step, ema_op]): train_op = tf.no_op(name=&apos;train&apos;) saver = tf.train.Saver() with tf.Session() as sess: init_op = tf.global_variables_initializer() sess.run(init_op) ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH) if ckpt and ckpt.model_checkpoint_path: saver.restore(sess, ckpt.model_checkpoint_path) for i in range(STEPS): xs, ys = mnist.train.next_batch(BATCH_SIZE) reshaped_xs = np.reshape(xs,( #同样变形 BATCH_SIZE, mnist_lenet5_forward.IMAGE_SIZE, mnist_lenet5_forward.IMAGE_SIZE, mnist_lenet5_forward.NUM_CHANNELS)) _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict=&#123;x: reshaped_xs, y_: ys&#125;) if i % 100 == 0: print(&quot;After %d training step(s), loss on training batch is %g.&quot; % (step, loss_value)) saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)def main(): mnist = input_data.read_data_sets(&quot;./data/&quot;, one_hot=True) backward(mnist)if __name__ == &apos;__main__&apos;: main() 测试程序中：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#coding:utf-8import timeimport tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataimport mnist_lenet5_forwardimport mnist_lenet5_backwardimport numpy as npTEST_INTERVAL_SECS = 5def test(mnist): with tf.Graph().as_default() as g: x = tf.placeholder(tf.float32,[ mnist.test.num_examples, mnist_lenet5_forward.IMAGE_SIZE, mnist_lenet5_forward.IMAGE_SIZE, mnist_lenet5_forward.NUM_CHANNELS]) y_ = tf.placeholder(tf.float32, [None, mnist_lenet5_forward.OUTPUT_NODE]) y = mnist_lenet5_forward.forward(x,False,None) #不使用dropout ema = tf.train.ExponentialMovingAverage(mnist_lenet5_backward.MOVING_AVERAGE_DECAY) ema_restore = ema.variables_to_restore() saver = tf.train.Saver(ema_restore) correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) while True: with tf.Session() as sess: ckpt = tf.train.get_checkpoint_state(mnist_lenet5_backward.MODEL_SAVE_PATH) if ckpt and ckpt.model_checkpoint_path: saver.restore(sess, ckpt.model_checkpoint_path) global_step = ckpt.model_checkpoint_path.split(&apos;/&apos;)[-1].split(&apos;-&apos;)[-1] reshaped_x = np.reshape(mnist.test.images,( mnist.test.num_examples, mnist_lenet5_forward.IMAGE_SIZE, mnist_lenet5_forward.IMAGE_SIZE, mnist_lenet5_forward.NUM_CHANNELS)) accuracy_score = sess.run(accuracy, feed_dict=&#123;x:reshaped_x,y_:mnist.test.labels&#125;) print(&quot;After %s training step(s), test accuracy = %g&quot; % (global_step, accuracy_score)) else: print(&apos;No checkpoint file found&apos;) return time.sleep(TEST_INTERVAL_SECS) def main(): mnist = input_data.read_data_sets(&quot;./data/&quot;, one_hot=True) test(mnist)if __name__ == &apos;__main__&apos;: main()]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu之apt-get与pip的区别]]></title>
    <url>%2F2018%2F04%2F19%2Fubuntu%E4%B9%8Bapt-get%E4%B8%8Epip%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[摘录：在下载Python依赖包的时候，突然发现有些人的执行命令是sudo pip install pysftp,也有人用sudo apt-get install pysftp,所以很好奇它们这两者到底区别在哪儿？什么时候该用pip,什么时候该用apt-get呢？该如何安装和使用apt-get与pip?现在，我就把自己学到的干货和大家分享下： pip与apt-get的区别在哪儿呢？ pip用来安装来自PyPI的python所有的依赖包，并且可以选择安装任何在PyPI上已上传的先前版本的依赖包；apt-get可以用来安装软件、更新源、也可以用来更新自Ubuntu的典型依赖包，典型安装即意味着它只是安装（最新发布的，或最近一个的）单一版本，并且我们不能决定我们要安装的依赖包的版本或选择它之前的版本。 什么时候该用pip,什么时候该用apt-get呢？ 情况是这样的，如果你需要最新版本的python依赖包，你可以直接使用apt-get,在项目突然需要使用旧版本的依赖包时，你就可以使用virtualenv和pip来使完美得再安装上一个旧版本的依赖包； 你可以根据你的喜好选择你喜欢的安装方式。但是，如果你需要安装python包的特定版本，或在virtualenv中安装包，或安装只托管PyPI上一个包，此时只有pip会帮你解决这个问题了。否则，如果你不介意安装在系统范围的位置使用apt-get或pip都是ok的； 该如何安装和使用apt-get与pip? apt-get是可以直接使用的，格式为sudo apt-get install/delete packagesudo apt-get -f install #修复安装sudo apt-get dist-upgrade #升级系统sudo apt-get upgrade #更新已安装的包apt-get source package #下载该包的源代码sudo apt-get build-dep package #安装相关的编译环境sudo apt-get clean &amp;&amp; sudo apt-get autoclean #清理无用的包 pip需要安装才能使用，配合virtualenv会锦上添花。安装过程如下（适用Ubuntu 10.10及以上版本），使用格式为：pip install package。 $ sudo apt-get install python-pip python-dev build-essential$ sudo pip install –upgrade pip$ sudo pip install –upgrade virtualenv 注：virtualenv是一个python工具，它可以创建一个独立的python环境，这样做的好处是你的python程序运行在这个环境里，不受其它python library的版本问题影响。]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow学习笔记整合]]></title>
    <url>%2F2018%2F04%2F18%2Ftensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E5%90%88%2F</url>
    <content type="text"><![CDATA[在慕课网上跟着学习了一段时间的《TensorFlow学习笔记》，将笔记整合如下：参考资料–慕课网：https://www.icourse163.org/learn/PKU-1002536002?tid=1002700003#/learn/content 基于tensorflow的神经网络：用张量表示数据，用计算图搭建网络，会话执行计算图，优化参数得到模型。 神经网络的实现过程： 准备数据集，提取特征，喂给神经网络（feed_dict） 搭建神经网络，从输入到输出（搭建计算图，会话执行）（前向传播） 大量特征数据喂给神经网络，迭代优化参数（反向传播） 使用训练好的模型。 神经网络的优化 激活函数：reLu，sigmoid,tanh 损失函数（Loss）：（y_与y之间的差距。）均方误差，交叉熵自定义，交叉熵， softmax函数： 学习率的控制：指数衰减学习率，（学习率初始设置为0.1，学习率衰减率为0.99，1轮后更新学习率） 滑动平均：（一段时间内的w，b的平均值） 正则化(为了减少过拟合)使用以上方法：神经网络模型的保存神经网络模型的加载评估断点续训：进行应用（applicatio）：优化总结前向函数：搭建神经网络，用到了 参数（w）正则化。关于正则化：123456W1 = tf.get_variable(&apos;weights_1&apos;,shape,tf.random_normal_initializer()) # 创建权重矩阵W1 tf.add_to_collection(&apos;losses&apos;,regularizer(W1)) # 将权重矩阵W1对应的正则项加入集合losses W2 = tf.get_variable(&apos;weights_2&apos;,shape,tf.random_normal_initializer()) # 创建权重矩阵W2 tf.add_to_collection(&apos;losses&apos;,regularizer(W2)) # 将权重矩阵W2对应的正则项加入集合losses ... Wn = tf.get_variable(&apos;weights_n&apos;,shape,tf.random_normal_initializer()) # 创建权重矩阵Wn tf.add_to_collection(&apos;losses&apos;,regularizer(Wn)) # 将权重矩阵Wn对应的正则项加入集合losses losses_collection = tf.get_collection(&apos;losses&apos;) # 以列表的形式获取集合losses中的值，每一个值都作为列表中的一个元素 loss = tf.add_n(losses_collection,name=&apos;loss&apos;) # 计算列表中所有元素之和，得到的结果就是正则项的值#使用时def get_weight(shape,regularizer): w = tf.Variable(tf.truncated_normal(shape,stddev=0.1)) if regularizer != None: tf.add_to_collection(&apos;losses&apos;,tf.contrib.layers.12_regularizer(regularizer)(w)) return(w) 反向函数：引入前向函数，Loss，用到了正则化，指数衰减，滑动平均。测试模型：测试准确性和泛化性。tf.Graph()复现计算图]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[abstract]]></title>
    <url>%2F2018%2F04%2F10%2Fabstract%2F</url>
    <content type="text"><![CDATA[2018（非专业方向）书籍阅读：《金阁寺》三岛由纪夫《数学之美》吴军《刺杀骑士团长》村上春树]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归与迭代]]></title>
    <url>%2F2017%2F12%2F27%2F%E9%80%92%E5%BD%92%E4%B8%8E%E8%BF%AD%E4%BB%A3%2F</url>
    <content type="text"><![CDATA[分清简单的概念 递归的基本概念:程序调用自身的编程技巧称为递归,是函数自己调用自己. 一个函数在其定义中直接或间接调用自身的一种方法,它通常把一个大型的复杂的问题转化为一个与原问题相似的规模较小的问题来解决,可以极大的减少代码量.递归的能力在于用有限的语句来定义对象的无限集合. 使用递归要注意的有两点: 1)递归就是在过程或函数里面调用自身; 2)在使用递归时,必须有一个明确的递归结束条件,称为递归出口. 递归分为两个阶段: 1)递推:把复杂的问题的求解推到比原问题简单一些的问题的求解; 2)回归:当获得最简单的情况后,逐步返回,依次得到复杂的解. 利用递归可以解决很多问题:如背包问题,汉诺塔问题,…等. 斐波那契数列为:0,1,1,2,3,5… fib(0)=0; fib(1)=1; fib(n)=fib(n-1)+fib(n-2); 迭代:利用变量的原值推算出变量的一个新值.如果递归是自己调用自己的话,迭代就是A不停的调用B. 递归中一定有迭代,但是迭代中不一定有递归,大部分可以相互转换.能用迭代的不用递归,递归调用函数,浪费空间,并且递归太深容易造成堆栈的溢出.c：12345678910111213141516//这是递归int funcA(int n)&#123; if(n &gt; 1) return n+funcA(n-1); else return 1;&#125;//这是迭代int funcB(int n)&#123; int i,s=0; for(i=1;i&lt;n;i++) s+=i; return s;&#125;]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
</search>
